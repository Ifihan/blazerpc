{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"BlazeRPC","text":"<p>A lightweight, framework-agnostic gRPC library for serving machine learning models in Python.</p> <p>BlazeRPC gives you a FastAPI-like developer experience: decorate a function, start the server, and you have a production-ready gRPC inference endpoint. No handwritten <code>.proto</code> files, no boilerplate servicers, no glue code.</p>"},{"location":"#what-it-does","title":"What it does","text":"<p>You write a plain Python function with type annotations. BlazeRPC turns it into a fully operational gRPC service.</p> <pre><code>from blazerpc import BlazeApp\n\napp = BlazeApp()\n\n@app.model(\"sentiment\")\ndef predict_sentiment(text: list[str]) -&gt; list[float]:\n    return model.predict(text)\n</code></pre> <pre><code>blaze serve app:app\n</code></pre> <p>That single command:</p> <ol> <li>Inspects your function's type annotations.</li> <li>Generates a <code>.proto</code> schema with matching request/response messages.</li> <li>Builds a gRPC servicer that routes requests to your function.</li> <li>Starts an async gRPC server with health checks and reflection.</li> </ol>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>Decorator-based API -- Register models with <code>@app.model(\"name\")</code>, just like route handlers in a web framework.</li> <li>Automatic proto generation -- BlazeRPC reads your type annotations and produces a valid <code>.proto</code> file. No hand-written schemas.</li> <li>Adaptive batching -- Individual requests are grouped into batches for GPU-efficient inference. Configurable batch size and timeout.</li> <li>Server-side streaming -- Return tokens one at a time with <code>streaming=True</code>, ideal for LLM inference.</li> <li>Health checks and reflection -- Built-in gRPC health checking protocol and server reflection, compatible with <code>grpcurl</code>, <code>grpcui</code>, and Kubernetes probes.</li> <li>Framework integrations -- Optional helpers for PyTorch, TensorFlow, and ONNX Runtime that handle tensor conversion automatically.</li> <li>Prometheus metrics -- Request counts and latencies are exported out of the box.</li> </ul>"},{"location":"#quick-links","title":"Quick links","text":"<ul> <li>Getting started -- Installation and your first BlazeRPC service.</li> <li>Architecture -- How the internals fit together.</li> <li>Configuration -- All <code>BlazeApp</code> parameters, CLI flags, and tuning knobs.</li> <li>API reference -- Every public class, function, and exception.</li> <li>Contributing -- How to set up a development environment and submit changes.</li> </ul>"},{"location":"api-reference/","title":"API reference","text":"<p>This page documents every public class, function, and exception in BlazeRPC.</p>"},{"location":"api-reference/#blazerpcblazeapp","title":"<code>blazerpc.BlazeApp</code>","text":"<p>The main entry point. Creates an application, registers models, and starts the server.</p> <pre><code>from blazerpc import BlazeApp\n\napp = BlazeApp(\n    name=\"my-service\",\n    enable_batching=True,\n    max_batch_size=32,\n    batch_timeout_ms=10.0,\n)\n</code></pre>"},{"location":"api-reference/#constructor","title":"Constructor","text":"Parameter Type Default Description <code>name</code> <code>str</code> <code>\"blazerpc\"</code> Application name (used in logging and diagnostics). <code>enable_batching</code> <code>bool</code> <code>True</code> Enable adaptive request batching. <code>max_batch_size</code> <code>int</code> <code>32</code> Maximum number of requests in a single batch. <code>batch_timeout_ms</code> <code>float</code> <code>10.0</code> Maximum time (ms) to wait before dispatching a partial batch."},{"location":"api-reference/#appmodelname-version1-streamingfalse","title":"<code>app.model(name, version=\"1\", streaming=False)</code>","text":"<p>Decorator that registers a function as a model endpoint.</p> Parameter Type Default Description <code>name</code> <code>str</code> required Model name. Becomes part of the RPC method name (<code>PredictName</code>). <code>version</code> <code>str</code> <code>\"1\"</code> Model version string. <code>streaming</code> <code>bool</code> <code>False</code> If <code>True</code>, the function must be an async generator that yields responses. <pre><code>@app.model(\"sentiment\", version=\"2\")\ndef predict_sentiment(text: list[str]) -&gt; list[float]:\n    return model.predict(text)\n</code></pre>"},{"location":"api-reference/#await-appservehost0000-port50051","title":"<code>await app.serve(host=\"0.0.0.0\", port=50051)</code>","text":"<p>Start the gRPC server and block until a shutdown signal is received. Registers the inference servicer, health service, and reflection handlers automatically.</p> Parameter Type Default Description <code>host</code> <code>str</code> <code>\"0.0.0.0\"</code> Bind address. <code>port</code> <code>int</code> <code>50051</code> Listen port."},{"location":"api-reference/#appregistry","title":"<code>app.registry</code>","text":"<p>The underlying <code>ModelRegistry</code> instance. Useful for introspection:</p> <pre><code>for model in app.registry.list_models():\n    print(f\"{model.name} v{model.version}\")\n</code></pre>"},{"location":"api-reference/#blazerpcblazeclient","title":"<code>blazerpc.BlazeClient</code>","text":"<p>Async gRPC client for calling BlazeRPC model endpoints.</p> <pre><code>from blazerpc import BlazeClient\n\nasync with BlazeClient(\"127.0.0.1\", 50051) as client:\n    result = await client.predict(\"echo\", text=\"hello\")\n\n    async for chunk in client.stream(\"tokens\", prompt=\"hi\"):\n        print(chunk)\n</code></pre>"},{"location":"api-reference/#constructor_1","title":"Constructor","text":"Parameter Type Default Description <code>host</code> <code>str</code> <code>\"127.0.0.1\"</code> Server address. <code>port</code> <code>int</code> <code>50051</code> Server port."},{"location":"api-reference/#await-clientpredictmodel_name-kwargs","title":"<code>await client.predict(model_name, **kwargs)</code>","text":"<p>Make a unary prediction call. Returns the model's result value.</p> Parameter Type Description <code>model_name</code> <code>str</code> The registered model name. <code>**kwargs</code> <code>Any</code> Input fields passed as JSON to the model."},{"location":"api-reference/#async-for-chunk-in-clientstreammodel_name-kwargs","title":"<code>async for chunk in client.stream(model_name, **kwargs)</code>","text":"<p>Make a server-streaming call. Yields each chunk's result value.</p> Parameter Type Description <code>model_name</code> <code>str</code> The registered model name. <code>**kwargs</code> <code>Any</code> Input fields passed as JSON to the model."},{"location":"api-reference/#clientclose","title":"<code>client.close()</code>","text":"<p>Close the underlying gRPC channel. Also called automatically when using the async context manager.</p>"},{"location":"api-reference/#blazerpctensorinput","title":"<code>blazerpc.TensorInput</code>","text":"<p>Type annotation for tensor-typed model inputs. Used by the codegen layer to emit <code>TensorProto</code> fields.</p> <pre><code>from blazerpc import TensorInput\nimport numpy as np\n\ndef classify(image: TensorInput[np.float32, \"batch\", 224, 224, 3]) -&gt; ...:\n    ...\n</code></pre> <p>The subscript arguments are <code>dtype</code> followed by shape dimensions. Shape dimensions can be integers or strings (symbolic names like <code>\"batch\"</code>).</p>"},{"location":"api-reference/#blazerpctensoroutput","title":"<code>blazerpc.TensorOutput</code>","text":"<p>Type annotation for tensor-typed model outputs. Same subscript syntax as <code>TensorInput</code>.</p> <pre><code>from blazerpc import TensorOutput\nimport numpy as np\n\ndef classify(...) -&gt; TensorOutput[np.float32, \"batch\", 1000]:\n    ...\n</code></pre>"},{"location":"api-reference/#exceptions","title":"Exceptions","text":"<p>All exceptions inherit from <code>BlazeRPCError</code>.</p>"},{"location":"api-reference/#blazerpcerror","title":"<code>BlazeRPCError</code>","text":"<p>Base exception for all BlazeRPC errors.</p>"},{"location":"api-reference/#validationerror","title":"<code>ValidationError</code>","text":"<p>Raised when input validation fails (bad shapes, types, missing annotations).</p> Attribute Type Description <code>field</code> <code>str \\| None</code> The field that failed validation."},{"location":"api-reference/#modelnotfounderror","title":"<code>ModelNotFoundError</code>","text":"<p>Raised when a requested model is not found in the registry.</p> Attribute Type Description <code>name</code> <code>str</code> Model name. <code>version</code> <code>str</code> Model version."},{"location":"api-reference/#serializationerror","title":"<code>SerializationError</code>","text":"<p>Raised when tensor serialization or deserialization fails.</p> Attribute Type Description <code>dtype</code> <code>str \\| None</code> The dtype that caused the error."},{"location":"api-reference/#inferenceerror","title":"<code>InferenceError</code>","text":"<p>Raised when model inference fails.</p> Attribute Type Description <code>model_name</code> <code>str \\| None</code> The model that failed."},{"location":"api-reference/#configurationerror","title":"<code>ConfigurationError</code>","text":"<p>Raised for invalid configuration (bad import paths, missing settings).</p>"},{"location":"api-reference/#serialization","title":"Serialization","text":""},{"location":"api-reference/#blazerpcruntimeserializationtensorproto","title":"<code>blazerpc.runtime.serialization.TensorProto</code>","text":"<p>Wire representation of a tensor. A dataclass with <code>__slots__</code>.</p> Field Type Description <code>shape</code> <code>tuple[int, ...]</code> Tensor shape. <code>dtype</code> <code>str</code> Proto type string (e.g. <code>\"float\"</code>, <code>\"int64\"</code>). <code>data</code> <code>bytes</code> Raw tensor bytes."},{"location":"api-reference/#serialize_tensorarr-npndarray-tensorproto","title":"<code>serialize_tensor(arr: np.ndarray) -&gt; TensorProto</code>","text":"<p>Serialize a NumPy array to a <code>TensorProto</code>. Raises <code>SerializationError</code> if the dtype is unsupported.</p>"},{"location":"api-reference/#deserialize_tensorproto-tensorproto-npndarray","title":"<code>deserialize_tensor(proto: TensorProto) -&gt; np.ndarray</code>","text":"<p>Deserialize a <code>TensorProto</code> back to a NumPy array. Uses <code>np.frombuffer()</code> for zero-copy reconstruction.</p>"},{"location":"api-reference/#server","title":"Server","text":""},{"location":"api-reference/#blazerpcservergrpcgrpcserver","title":"<code>blazerpc.server.grpc.GRPCServer</code>","text":"<p>Production-ready async gRPC server. Wraps <code>grpclib.server.Server</code> with signal handling and graceful shutdown.</p> <pre><code>server = GRPCServer(handlers, grace_period=5.0)\nawait server.start(host=\"0.0.0.0\", port=50051)\n</code></pre> Constructor parameter Type Default Description <code>handlers</code> <code>Sequence[Any]</code> required List of grpclib-compatible handlers. <code>grace_period</code> <code>float</code> <code>5.0</code> Seconds to wait for in-flight requests during shutdown."},{"location":"api-reference/#build_health_serviceservicersnone-health","title":"<code>build_health_service(servicers=None) -&gt; Health</code>","text":"<p>Create a gRPC health service. Pass servicer instances for per-service health tracking, or <code>None</code> for unconditional <code>SERVING</code> status.</p>"},{"location":"api-reference/#build_reflection_servicehandlersnone-list","title":"<code>build_reflection_service(handlers=None) -&gt; list</code>","text":"<p>Create gRPC reflection handlers. Pass gRPC service handler objects (e.g. the servicer from <code>build_servicer()</code>) so clients can discover available RPCs.</p>"},{"location":"api-reference/#middleware","title":"Middleware","text":""},{"location":"api-reference/#blazerpcservermiddlewaremiddleware","title":"<code>blazerpc.server.middleware.Middleware</code>","text":"<p>Abstract base class for server middleware. Subclass it and implement <code>on_request()</code> and <code>on_response()</code>.</p> <pre><code>class MyMiddleware(Middleware):\n    async def on_request(self, event: RecvRequest) -&gt; None:\n        ...\n\n    async def on_response(self, event: SendTrailingMetadata) -&gt; None:\n        ...\n</code></pre> <p>Call <code>middleware.attach(server)</code> to register it on a <code>grpclib.server.Server</code> instance.</p>"},{"location":"api-reference/#loggingmiddleware","title":"<code>LoggingMiddleware</code>","text":"<p>Logs every RPC call with method name, peer address, and response status.</p>"},{"location":"api-reference/#metricsmiddleware","title":"<code>MetricsMiddleware</code>","text":"<p>Exports Prometheus metrics:</p> <ul> <li><code>blazerpc_requests_total{method, status}</code> -- Counter of total requests.</li> <li><code>blazerpc_request_duration_seconds{method}</code> -- Histogram of request durations.</li> </ul>"},{"location":"api-reference/#exceptionmiddleware","title":"<code>ExceptionMiddleware</code>","text":"<p>Base class for custom exception-to-gRPC-status mapping. A no-op by default.</p>"},{"location":"api-reference/#runtime","title":"Runtime","text":""},{"location":"api-reference/#blazerpcruntimebatcherbatcher","title":"<code>blazerpc.runtime.batcher.Batcher</code>","text":"<p>Adaptive request batcher. Collects individual requests into batches.</p> Constructor parameter Type Default Description <code>max_size</code> <code>int</code> <code>32</code> Maximum items per batch. <code>timeout_ms</code> <code>float</code> <code>10.0</code> Max wait time (ms) before dispatching. <p>Key methods:</p> <ul> <li><code>await submit(request)</code> -- Submit a request and wait for the batched result.</li> <li><code>await start(inference_fn)</code> -- Start the background batching loop.</li> <li><code>await stop()</code> -- Stop the batching loop.</li> </ul>"},{"location":"api-reference/#blazerpcruntimeexecutormodelexecutor","title":"<code>blazerpc.runtime.executor.ModelExecutor</code>","text":"<p>Wraps a registered model function with sync/async bridging.</p> <ul> <li><code>await execute(kwargs)</code> -- Run inference with keyword arguments.</li> <li><code>await execute_batch(kwargs_list)</code> -- Run inference on a batch of inputs.</li> </ul> <p>Synchronous model functions are offloaded to a thread pool via <code>asyncio.to_thread()</code>.</p>"},{"location":"api-reference/#code-generation","title":"Code generation","text":""},{"location":"api-reference/#blazerpccodegenprotoprotogenerator","title":"<code>blazerpc.codegen.proto.ProtoGenerator</code>","text":"<p>Generates <code>.proto</code> file content from a <code>ModelRegistry</code>.</p> <pre><code>from blazerpc.codegen.proto import ProtoGenerator\n\nproto_content = ProtoGenerator().generate(app.registry)\n</code></pre>"},{"location":"api-reference/#blazerpccodegenservicerbuild_servicerregistry-batchersnone","title":"<code>blazerpc.codegen.servicer.build_servicer(registry, batchers=None)</code>","text":"<p>Builds a grpclib-compatible <code>InferenceServicer</code> from a <code>ModelRegistry</code>.</p> <pre><code>from blazerpc.codegen.servicer import build_servicer\n\nservicer = build_servicer(app.registry, batchers={\"my_model\": batcher})\n</code></pre>"},{"location":"api-reference/#contrib","title":"Contrib","text":""},{"location":"api-reference/#blazerpccontribpytorch","title":"<code>blazerpc.contrib.pytorch</code>","text":"Function / Decorator Description <code>torch_to_numpy(tensor) -&gt; np.ndarray</code> Detach, move to CPU, and convert to NumPy. <code>numpy_to_torch(arr, device, dtype) -&gt; Tensor</code> Convert NumPy array to a PyTorch tensor. <code>@torch_model(device=\"cpu\")</code> Decorator that auto-converts inputs and outputs."},{"location":"api-reference/#blazerpccontribtensorflow","title":"<code>blazerpc.contrib.tensorflow</code>","text":"Function / Decorator Description <code>tf_to_numpy(tensor) -&gt; np.ndarray</code> Convert a TensorFlow tensor to NumPy. <code>numpy_to_tf(arr, dtype) -&gt; tf.Tensor</code> Convert NumPy array to a TensorFlow tensor. <code>@tf_model(dtype=None)</code> Decorator that auto-converts inputs and outputs."},{"location":"api-reference/#blazerpccontribonnxonnxmodel","title":"<code>blazerpc.contrib.onnx.ONNXModel</code>","text":"<p>Wrapper around an ONNX Runtime inference session.</p> <pre><code>from blazerpc.contrib.onnx import ONNXModel\n\nmodel = ONNXModel(\"model.onnx\", providers=[\"CUDAExecutionProvider\"])\nresults = model.predict(input_array)\n</code></pre> Constructor parameter Type Default Description <code>model_path</code> <code>str \\| Path</code> required Path to the <code>.onnx</code> file. <code>providers</code> <code>list[str] \\| None</code> <code>[\"CPUExecutionProvider\"]</code> ONNX Runtime execution providers. <code>session_options</code> <code>Any</code> <code>None</code> Optional <code>ort.SessionOptions</code>. <p>Methods:</p> <ul> <li><code>predict(*inputs) -&gt; list[np.ndarray]</code> -- Run inference with positional inputs matched to input names.</li> <li><code>predict_dict(inputs) -&gt; dict[str, np.ndarray]</code> -- Run inference with named inputs, returning named outputs.</li> <li><code>input_names -&gt; list[str]</code> -- Names of the model's input tensors.</li> <li><code>output_names -&gt; list[str]</code> -- Names of the model's output tensors.</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>This page describes how BlazeRPC's internals fit together. Understanding the architecture is useful if you want to contribute, debug unexpected behavior, or extend the library with custom components.</p>"},{"location":"architecture/#high-level-overview","title":"High-level overview","text":"<p>A BlazeRPC application moves through three phases: registration, code generation, and serving.</p> <pre><code>flowchart LR\n    A[\"@app.model\\ndecorators\"] --&gt; B[\"ModelRegistry\\n(stores metadata)\"]\n    B --&gt; C[\"ProtoGen\\n+ Servicer Gen\"]\n    C --&gt; D[\"GRPCServer\\n+ Health\\n+ Reflection\\n+ Middleware\"]</code></pre>"},{"location":"architecture/#1-registration","title":"1. Registration","text":"<p>When you write <code>@app.model(\"sentiment\")</code>, the decorator calls <code>ModelRegistry.register()</code>. The registry stores a <code>ModelInfo</code> dataclass for each model containing:</p> <ul> <li>The function reference.</li> <li>The model name and version.</li> <li>Input types and output type extracted from the function's type annotations via <code>extract_type_info()</code>.</li> <li>Whether the model is a streaming endpoint.</li> </ul> <p>No protobuf code is generated at this stage. Registration is pure metadata collection.</p>"},{"location":"architecture/#2-code-generation","title":"2. Code generation","text":"<p>When the server starts (or when you run <code>blaze proto</code>), BlazeRPC generates two things from the registry:</p> <p>Proto schema. <code>ProtoGenerator</code> walks the registry and produces a <code>.proto</code> file. Each model becomes a request message, a response message, and an RPC method on <code>InferenceService</code>. Python types are mapped to proto types via <code>PYTHON_TYPE_MAP</code> and <code>DTYPE_MAP</code>. Streaming models produce <code>returns (stream Response)</code> RPCs.</p> <p>Servicer. <code>InferenceServicer</code> implements the <code>__mapping__()</code> protocol that grpclib expects. For each model, it creates a handler function that:</p> <ol> <li>Receives the raw request from the gRPC stream.</li> <li>Decodes it into keyword arguments.</li> <li>Calls the model function (via <code>asyncio.to_thread()</code> if it is synchronous).</li> <li>Encodes the result and sends it back.</li> </ol> <p>Streaming models use async generators -- each <code>yield</code> sends one response message over the open stream.</p>"},{"location":"architecture/#3-serving","title":"3. Serving","text":"<p><code>BlazeApp.serve()</code> assembles the full handler stack:</p> <ol> <li>Builds an <code>InferenceServicer</code> from the registry.</li> <li>Creates a <code>Health</code> service (gRPC health checking protocol).</li> <li>Creates reflection handlers so tools like <code>grpcurl</code> can discover the API.</li> <li>Passes all handlers to <code>GRPCServer</code>, which wraps <code>grpclib.server.Server</code>.</li> </ol> <p>The server installs signal handlers for <code>SIGINT</code> and <code>SIGTERM</code> and blocks until one of them fires. On shutdown, it closes the listener and waits up to a configurable grace period for in-flight requests to complete.</p>"},{"location":"architecture/#module-map","title":"Module map","text":"<pre><code>src/blazerpc/\n\u251c\u2500\u2500 __init__.py              # Public API exports\n\u251c\u2500\u2500 app.py                   # BlazeApp -- the entry point\n\u251c\u2500\u2500 types.py                 # TensorInput, TensorOutput, type introspection\n\u251c\u2500\u2500 exceptions.py            # Exception hierarchy\n\u2502\n\u251c\u2500\u2500 runtime/\n\u2502   \u251c\u2500\u2500 registry.py          # ModelRegistry, ModelInfo dataclass\n\u2502   \u251c\u2500\u2500 executor.py          # ModelExecutor (sync/async bridging)\n\u2502   \u251c\u2500\u2500 batcher.py           # Adaptive batching with partial failure handling\n\u2502   \u2514\u2500\u2500 serialization.py     # Tensor and scalar serialization\n\u2502\n\u251c\u2500\u2500 codegen/\n\u2502   \u251c\u2500\u2500 proto.py             # .proto file generation\n\u2502   \u2514\u2500\u2500 servicer.py          # Dynamic grpclib servicer generation\n\u2502\n\u251c\u2500\u2500 server/\n\u2502   \u251c\u2500\u2500 grpc.py              # GRPCServer with signal handling and graceful shutdown\n\u2502   \u251c\u2500\u2500 health.py            # gRPC health checking protocol\n\u2502   \u251c\u2500\u2500 reflection.py        # gRPC server reflection\n\u2502   \u2514\u2500\u2500 middleware.py        # Logging, metrics, and extensible middleware base\n\u2502\n\u251c\u2500\u2500 contrib/\n\u2502   \u251c\u2500\u2500 pytorch.py           # PyTorch &lt;-&gt; NumPy helpers and @torch_model\n\u2502   \u251c\u2500\u2500 tensorflow.py        # TensorFlow &lt;-&gt; NumPy helpers and @tf_model\n\u2502   \u2514\u2500\u2500 onnx.py              # ONNX Runtime session wrapper\n\u2502\n\u2514\u2500\u2500 cli/\n    \u251c\u2500\u2500 main.py              # Typer CLI entry point (blaze serve, blaze proto)\n    \u251c\u2500\u2500 serve.py             # App loading from import strings\n    \u2514\u2500\u2500 proto.py             # Proto file export logic\n</code></pre>"},{"location":"architecture/#request-lifecycle","title":"Request lifecycle","text":"<p>Here is the full path of a unary (non-streaming) request:</p> <ol> <li>Client sends request over gRPC to <code>blazerpc.InferenceService/PredictSentiment</code>.</li> <li>grpclib matches the path to the handler registered in <code>InferenceServicer.__mapping__()</code>.</li> <li>Handler calls <code>stream.recv_message()</code> to read the request bytes.</li> <li>Decode -- <code>_decode_request()</code> converts the raw message into keyword arguments.</li> <li>Batching (optional) -- If batching is enabled, the request is submitted to the <code>Batcher</code> queue. The batcher collects requests until <code>max_batch_size</code> is reached or <code>batch_timeout_ms</code> expires, then calls the model function with the full batch.</li> <li>Execution -- The model function runs. If it is synchronous, it is offloaded to a thread pool with <code>asyncio.to_thread()</code> so it does not block the event loop.</li> <li>Encode -- <code>_encode_response()</code> converts the result to its wire representation. NumPy arrays become <code>TensorProto</code> messages.</li> <li>Handler calls <code>stream.send_message()</code> to send the response.</li> </ol> <p>For streaming models, steps 6-8 repeat for each yielded value.</p>"},{"location":"architecture/#adaptive-batching-internals","title":"Adaptive batching internals","text":"<p>The <code>Batcher</code> runs as a background <code>asyncio.Task</code>. Its loop:</p> <ol> <li>Waits for the first item to arrive in the queue.</li> <li>Collects additional items until either <code>max_batch_size</code> is reached or <code>batch_timeout_ms</code> elapses.</li> <li>Calls the inference function with the collected batch.</li> <li>Distributes results back to individual futures.</li> </ol> <p>If the inference function raises an exception, every future in the batch receives that exception. If the function returns an <code>Exception</code> instance at a specific index, only that item's future is rejected -- other items in the batch still receive their results. This is the partial failure model.</p>"},{"location":"architecture/#type-system","title":"Type system","text":"<p>BlazeRPC's type system bridges Python annotations and protobuf fields:</p> Python type Proto type Notes <code>str</code> <code>string</code> <code>int</code> <code>int64</code> <code>float</code> <code>float</code> <code>bool</code> <code>bool</code> <code>bytes</code> <code>bytes</code> <code>list[float]</code> <code>repeated float</code> Any <code>list[T]</code> is supported <code>TensorInput[np.float32, 224, 224]</code> <code>TensorProto</code> Shape and dtype metadata <code>TensorOutput[np.float32, 1000]</code> <code>TensorProto</code> <p><code>TensorInput</code> and <code>TensorOutput</code> are generic type annotations. At class-getitem time they produce a <code>_TensorType</code> instance that stores the dtype and shape. The codegen layer uses this metadata to emit <code>TensorProto</code> message fields and the serialization layer uses it to validate arrays at runtime.</p>"},{"location":"architecture/#middleware-system","title":"Middleware system","text":"<p>Middleware hooks into grpclib's event system rather than wrapping handlers. Each middleware subclass implements <code>on_request()</code> and <code>on_response()</code>, which are called via <code>RecvRequest</code> and <code>SendTrailingMetadata</code> events respectively.</p> <p>This design means middleware runs outside the handler function. It cannot modify request data, but it can inspect metadata, record timing, and observe response status codes -- which is exactly what logging and metrics middleware need.</p> <p>To write custom middleware, subclass <code>Middleware</code> and call <code>attach(server)</code> on the grpclib <code>Server</code> instance.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>This page lists every configurable parameter in BlazeRPC, organized by where it is set.</p>"},{"location":"configuration/#blazeapp-constructor","title":"<code>BlazeApp</code> constructor","text":"<p>These parameters are passed when you create the application instance.</p> <pre><code>from blazerpc import BlazeApp\n\napp = BlazeApp(\n    name=\"my-service\",\n    enable_batching=True,\n    max_batch_size=32,\n    batch_timeout_ms=10.0,\n)\n</code></pre> Parameter Type Default Description <code>name</code> <code>str</code> <code>\"blazerpc\"</code> Application name. Used in logging output and diagnostics. <code>enable_batching</code> <code>bool</code> <code>True</code> Enable adaptive request batching. Set to <code>False</code> for unbatched processing. <code>max_batch_size</code> <code>int</code> <code>32</code> Maximum number of requests collected into a single batch. <code>batch_timeout_ms</code> <code>float</code> <code>10.0</code> Maximum time (in milliseconds) to wait for a full batch before dispatching a partial one. Lower values reduce latency; higher values improve throughput."},{"location":"configuration/#appmodel-decorator","title":"<code>@app.model()</code> decorator","text":"<p>These parameters are passed per model.</p> <pre><code>@app.model(\"sentiment\", version=\"2\", streaming=False)\ndef predict_sentiment(text: list[str]) -&gt; list[float]:\n    ...\n</code></pre> Parameter Type Default Description <code>name</code> <code>str</code> required Model name. Converted to PascalCase for the RPC method name (e.g. <code>\"sentiment\"</code> becomes <code>PredictSentiment</code>). <code>version</code> <code>str</code> <code>\"1\"</code> Model version string. Stored as metadata; does not affect routing. <code>streaming</code> <code>bool</code> <code>False</code> If <code>True</code>, the function must be an async generator that yields responses. Produces a <code>returns (stream Response)</code> RPC."},{"location":"configuration/#grpcserver-constructor","title":"<code>GRPCServer</code> constructor","text":"<p>These parameters control the underlying gRPC server behavior. They are set internally by <code>BlazeApp.serve()</code> but can be used directly if you instantiate <code>GRPCServer</code> yourself.</p> <pre><code>from blazerpc.server.grpc import GRPCServer\n\nserver = GRPCServer(handlers, grace_period=5.0)\nawait server.start(host=\"0.0.0.0\", port=50051)\n</code></pre> Parameter Type Default Description <code>handlers</code> <code>Sequence[Any]</code> required List of grpclib-compatible handler objects (servicers). <code>grace_period</code> <code>float</code> <code>5.0</code> Seconds to wait for in-flight requests to complete during shutdown. After this period, the server shuts down forcefully."},{"location":"configuration/#serverstart-parameters","title":"<code>server.start()</code> parameters","text":"Parameter Type Default Description <code>host</code> <code>str</code> <code>\"0.0.0.0\"</code> Bind address. <code>port</code> <code>int</code> <code>50051</code> Listen port."},{"location":"configuration/#cli-options","title":"CLI options","text":""},{"location":"configuration/#blaze-serve","title":"<code>blaze serve</code>","text":"<pre><code>blaze serve &lt;app_path&gt; [OPTIONS]\n</code></pre> Argument / Option Type Default Description <code>app_path</code> <code>str</code> required App import path in <code>module:attribute</code> format (e.g. <code>app:app</code>). <code>--host</code> <code>str</code> <code>\"0.0.0.0\"</code> Host to bind to. <code>--port</code> <code>int</code> <code>50051</code> Port to listen on. <code>--workers</code> <code>int</code> <code>1</code> Number of worker processes. <code>--reload</code> <code>bool</code> <code>False</code> Enable auto-reload for development. Requires <code>watchfiles</code>."},{"location":"configuration/#hot-reload","title":"Hot reload","text":"<p>When <code>--reload</code> is enabled, the server watches for <code>.py</code> file changes in the current directory and automatically restarts when changes are detected. This uses process-level restart (like uvicorn) for a clean reimport of all modules.</p> <pre><code>blaze serve app:app --reload\n</code></pre> <p>Install the reload dependency:</p> <pre><code>pip install blazerpc[reload]\n# or\npip install watchfiles\n</code></pre> <p>The reload feature is intended for development only \u2014 do not use it in production.</p>"},{"location":"configuration/#blaze-proto","title":"<code>blaze proto</code>","text":"<pre><code>blaze proto &lt;app_path&gt; [OPTIONS]\n</code></pre> Argument / Option Type Default Description <code>app_path</code> <code>str</code> required App import path in <code>module:attribute</code> format. <code>--output-dir</code> <code>str</code> <code>\".\"</code> Output directory for the generated <code>.proto</code> file."},{"location":"configuration/#logging","title":"Logging","text":"<p>BlazeRPC uses Python's standard <code>logging</code> module. The <code>blaze serve</code> command configures <code>INFO</code>-level logging by default. Logger names follow the module hierarchy:</p> Logger name Source <code>blazerpc.server</code> Server lifecycle events. <code>blazerpc.batcher</code> Batch collection and dispatch. <code>blazerpc.middleware</code> Middleware event hooks. <code>blazerpc.reflection</code> Reflection service setup. <p>To customize logging, configure these loggers before calling <code>blaze serve</code> or <code>app.serve()</code>:</p> <pre><code>import logging\n\nlogging.getLogger(\"blazerpc.batcher\").setLevel(logging.DEBUG)\n</code></pre>"},{"location":"configuration/#prometheus-metrics","title":"Prometheus metrics","text":"<p>When <code>MetricsMiddleware</code> is attached, the following metrics are exported:</p> Metric name Type Labels Description <code>blazerpc_requests_total</code> Counter <code>method</code>, <code>status</code> Total number of gRPC requests. <code>blazerpc_request_duration_seconds</code> Histogram <code>method</code> Request duration in seconds. <p>These are standard <code>prometheus_client</code> objects. Expose them via a Prometheus scrape endpoint (e.g., using <code>prometheus_client.start_http_server()</code>).</p>"},{"location":"contributing/","title":"Contributing","text":"<p>For the full contributing guide -- including environment setup, code style conventions, testing instructions, and pull request guidelines -- see CONTRIBUTING.md in the repository root.</p>"},{"location":"contributing/#quick-reference","title":"Quick reference","text":"<pre><code># Clone and install\ngit clone https://github.com/Ifihan/blazerpc.git\ncd blazerpc\nuv sync --extra dev\n\n# Run tests\nuv run pytest tests/ -v\n\n# Lint\nuv run ruff check src/ tests/\n\n# Type check\nuv run mypy src/blazerpc/\n</code></pre>"},{"location":"getting-started/","title":"Getting started","text":"<p>This guide walks you through installing BlazeRPC, defining a model, starting the server, and exporting a <code>.proto</code> file.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code>pip install blazerpc\n</code></pre> <p>If you use a specific ML framework, install the corresponding extra:</p> <pre><code>pip install blazerpc[pytorch]      # PyTorch tensor conversion helpers\npip install blazerpc[tensorflow]   # TensorFlow tensor conversion helpers\npip install blazerpc[onnx]         # ONNX Runtime model wrapper\npip install blazerpc[all]          # All optional integrations\n</code></pre>"},{"location":"getting-started/#define-a-model","title":"Define a model","text":"<p>Create a file called <code>app.py</code>:</p> <pre><code>from blazerpc import BlazeApp\n\napp = BlazeApp()\n\n@app.model(\"sentiment\")\ndef predict_sentiment(text: list[str]) -&gt; list[float]:\n    # Replace with your real model inference\n    return [0.95] * len(text)\n</code></pre> <p>BlazeRPC reads the type annotations on your function to generate the gRPC request and response messages. Supported types include <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, <code>list[float]</code>, <code>list[str]</code>, and tensor types via <code>TensorInput</code> / <code>TensorOutput</code>.</p>"},{"location":"getting-started/#start-the-server","title":"Start the server","text":"<pre><code>blaze serve app:app\n</code></pre> <p>The import string follows the <code>module:attribute</code> convention. BlazeRPC imports the module, looks up the attribute, and starts the gRPC server.</p> <pre><code>\u26a1 BlazeRPC server starting...\n  \u2713 Loaded model: sentiment v1\n  \u2713 Server listening on 0.0.0.0:50051\n</code></pre> <p>The server registers three services automatically:</p> Service Purpose <code>blazerpc.InferenceService</code> Your model RPCs <code>grpc.health.v1.Health</code> Standard health checks <code>grpc.reflection.v1alpha.ServerReflection</code> Service discovery"},{"location":"getting-started/#export-the-proto-file","title":"Export the <code>.proto</code> file","text":"<pre><code>blaze proto app:app --output-dir ./proto_out\n</code></pre> <p>This writes a <code>blaze_service.proto</code> file that you can compile with <code>protoc</code> or share with clients in any language. The generated proto looks like this:</p> <pre><code>syntax = \"proto3\";\npackage blazerpc;\n\nmessage TensorProto {\n  repeated int64 shape = 1;\n  string dtype = 2;\n  bytes data = 3;\n}\n\nmessage SentimentRequest {\n  repeated string text = 1;\n}\n\nmessage SentimentResponse {\n  repeated float result = 1;\n}\n\nservice InferenceService {\n  rpc PredictSentiment(SentimentRequest) returns (SentimentResponse);\n}\n</code></pre>"},{"location":"getting-started/#multiple-models","title":"Multiple models","text":"<p>Register as many models as you need on the same app. Each model becomes its own RPC method:</p> <pre><code>@app.model(\"sentiment\")\ndef predict_sentiment(text: list[str]) -&gt; list[float]:\n    return [0.92] * len(text)\n\n@app.model(\"ner\")\ndef predict_ner(text: str) -&gt; list[str]:\n    return [\"BlazeRPC\", \"gRPC\", \"Python\"]\n\n@app.model(\"summarize\")\ndef summarize(text: str, max_length: int) -&gt; str:\n    return text[:max_length]\n</code></pre> <p>All three models are served under the same <code>InferenceService</code> and discovered through a single reflection endpoint.</p>"},{"location":"getting-started/#next-steps","title":"Next steps","text":"<ul> <li>Streaming -- Return tokens incrementally for LLM workloads.</li> <li>Adaptive batching -- Group requests into batches for GPU efficiency.</li> <li>Framework integrations -- Use PyTorch, TensorFlow, or ONNX Runtime with automatic tensor conversion.</li> <li>Configuration -- Tune batch size, timeouts, and server options.</li> </ul>"},{"location":"guides/batching/","title":"Adaptive batching","text":"<p>GPU inference is significantly faster when processing a batch of inputs at once compared to processing them one at a time. BlazeRPC's adaptive batcher collects individual requests from concurrent clients and groups them into batches before calling your model function.</p>"},{"location":"guides/batching/#enabling-batching","title":"Enabling batching","text":"<p>Batching is enabled by default. Configure it through <code>BlazeApp</code>:</p> <pre><code>from blazerpc import BlazeApp\n\napp = BlazeApp(\n    enable_batching=True,\n    max_batch_size=32,\n    batch_timeout_ms=10.0,\n)\n</code></pre> Parameter Default Description <code>enable_batching</code> <code>True</code> Set to <code>False</code> to process every request individually. <code>max_batch_size</code> <code>32</code> Maximum number of requests collected into a single batch. <code>batch_timeout_ms</code> <code>10.0</code> Maximum time (in milliseconds) to wait for a full batch before dispatching a partial one."},{"location":"guides/batching/#how-it-works","title":"How it works","text":"<p>The batcher runs as a background <code>asyncio.Task</code> with the following loop:</p> <ol> <li>Wait for the first request to arrive.</li> <li>Collect additional requests until either <code>max_batch_size</code> is reached or <code>batch_timeout_ms</code> elapses.</li> <li>Dispatch the batch to the model function.</li> <li>Distribute results back to each client's individual future.</li> </ol> <p>This means:</p> <ul> <li>Under high load, batches fill up quickly and are dispatched at full capacity.</li> <li>Under light load, the timeout ensures that a lone request is not stuck waiting for a batch to fill. A 10 ms timeout adds negligible latency.</li> </ul>"},{"location":"guides/batching/#example","title":"Example","text":"<pre><code>import numpy as np\nfrom blazerpc import BlazeApp\n\napp = BlazeApp(\n    enable_batching=True,\n    max_batch_size=16,\n    batch_timeout_ms=5.0,\n)\n\n@app.model(\"classify\")\ndef classify_image(image: list[float]) -&gt; float:\n    # In production, this would be a batch-aware model call.\n    return float(np.mean(image))\n</code></pre> <p>When three clients call <code>classify_image</code> within 5 ms of each other, the batcher groups all three requests into a single batch. The model function runs once, and each client receives only its own result.</p>"},{"location":"guides/batching/#tuning","title":"Tuning","text":"<p><code>max_batch_size</code> controls the upper bound on batch size. Set this based on your GPU memory and model throughput characteristics. Larger batches improve throughput but use more memory.</p> <p><code>batch_timeout_ms</code> controls latency under light load. Lower values reduce tail latency for individual requests. Higher values give the batcher more time to collect a full batch, improving throughput.</p> <p>A good starting point:</p> <ul> <li>For latency-sensitive applications (real-time APIs): <code>batch_timeout_ms=5.0</code>, <code>max_batch_size=8</code>.</li> <li>For throughput-optimized workloads (offline processing): <code>batch_timeout_ms=50.0</code>, <code>max_batch_size=64</code>.</li> </ul>"},{"location":"guides/batching/#partial-failure-handling","title":"Partial failure handling","text":"<p>If the model function raises an exception, every request in the batch receives that exception. This is the \"whole-batch failure\" case.</p> <p>BlazeRPC also supports per-item failure. If the model function returns an <code>Exception</code> instance at a specific index in the results list, only that item's request is rejected. Other items in the batch still receive their results:</p> <pre><code>@app.model(\"classify\")\ndef classify_batch(inputs: list[dict]) -&gt; list:\n    results = []\n    for inp in inputs:\n        try:\n            results.append(model.predict(inp))\n        except Exception as e:\n            results.append(e)  # This item fails; others succeed.\n    return results\n</code></pre> <p>If the results list has a different length than the input batch, every request receives a <code>RuntimeError</code> explaining the mismatch.</p>"},{"location":"guides/batching/#disabling-batching","title":"Disabling batching","text":"<p>Set <code>enable_batching=False</code> to process every request individually:</p> <pre><code>app = BlazeApp(enable_batching=False)\n</code></pre> <p>This is appropriate when:</p> <ul> <li>Your model does not benefit from batched inference (e.g., it processes one item at a time internally).</li> <li>You are using streaming endpoints (streaming is always unbatched).</li> <li>You want the simplest possible request path for debugging.</li> </ul>"},{"location":"guides/integrations/","title":"Framework integrations","text":"<p>BlazeRPC provides optional helpers for PyTorch, TensorFlow, and ONNX Runtime. These handle the conversion between NumPy arrays (BlazeRPC's wire format) and framework-specific tensor types so you can write model code in the framework's native API.</p>"},{"location":"guides/integrations/#pytorch","title":"PyTorch","text":"<p>Install the extra:</p> <pre><code>pip install blazerpc[pytorch]\n</code></pre>"},{"location":"guides/integrations/#torch_model-decorator","title":"<code>@torch_model</code> decorator","text":"<p>The <code>@torch_model</code> decorator converts NumPy inputs to PyTorch tensors before your function runs, and converts the PyTorch tensor output back to NumPy when it returns:</p> <pre><code>from blazerpc import BlazeApp\nfrom blazerpc.contrib.pytorch import torch_model\n\napp = BlazeApp()\n\n@app.model(\"classifier\")\n@torch_model(device=\"cuda\")\ndef classify(image):\n    # `image` is a torch.Tensor on CUDA\n    return model(image)\n    # Return value is converted back to np.ndarray automatically\n</code></pre> Parameter Type Default Description <code>device</code> <code>str</code> <code>\"cpu\"</code> Target device (<code>\"cpu\"</code>, <code>\"cuda\"</code>, <code>\"cuda:0\"</code>). <p>The decorator can be used with or without arguments:</p> <pre><code>@torch_model           # Uses default device=\"cpu\"\n@torch_model()         # Same as above\n@torch_model(device=\"cuda:1\")  # Specify a GPU\n</code></pre>"},{"location":"guides/integrations/#standalone-conversion-functions","title":"Standalone conversion functions","text":"<p>If you need more control, use the conversion functions directly:</p> <pre><code>from blazerpc.contrib.pytorch import torch_to_numpy, numpy_to_torch\n\n# NumPy -&gt; PyTorch\ntensor = numpy_to_torch(arr, device=\"cuda\", dtype=torch.float16)\n\n# PyTorch -&gt; NumPy (detaches from graph, moves to CPU)\narray = torch_to_numpy(tensor)\n</code></pre>"},{"location":"guides/integrations/#tensorflow","title":"TensorFlow","text":"<p>Install the extra:</p> <pre><code>pip install blazerpc[tensorflow]\n</code></pre>"},{"location":"guides/integrations/#tf_model-decorator","title":"<code>@tf_model</code> decorator","text":"<p>Works the same way as <code>@torch_model</code>, converting NumPy inputs to TensorFlow tensors and back:</p> <pre><code>from blazerpc.contrib.tensorflow import tf_model\n\n@app.model(\"classifier\")\n@tf_model\ndef classify(image):\n    # `image` is a tf.Tensor\n    return model(image)\n</code></pre> Parameter Type Default Description <code>dtype</code> <code>Any</code> <code>None</code> Optional TensorFlow dtype to cast inputs to."},{"location":"guides/integrations/#standalone-conversion-functions_1","title":"Standalone conversion functions","text":"<pre><code>from blazerpc.contrib.tensorflow import tf_to_numpy, numpy_to_tf\n\ntensor = numpy_to_tf(arr, dtype=tf.float16)\narray = tf_to_numpy(tensor)\n</code></pre>"},{"location":"guides/integrations/#onnx-runtime","title":"ONNX Runtime","text":"<p>Install the extra:</p> <pre><code>pip install blazerpc[onnx]\n</code></pre>"},{"location":"guides/integrations/#onnxmodel-wrapper","title":"<code>ONNXModel</code> wrapper","text":"<p><code>ONNXModel</code> manages an ONNX Runtime inference session and exposes a simple <code>predict()</code> method:</p> <pre><code>from blazerpc.contrib.onnx import ONNXModel\n\nonnx_model = ONNXModel(\n    \"model.onnx\",\n    providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"],\n)\n\n@app.model(\"classifier\")\ndef classify(image: np.ndarray) -&gt; np.ndarray:\n    return onnx_model.predict(image)[0]\n</code></pre> Constructor parameter Type Default Description <code>model_path</code> <code>str \\| Path</code> required Path to the <code>.onnx</code> file. <code>providers</code> <code>list[str] \\| None</code> <code>[\"CPUExecutionProvider\"]</code> Execution providers. <code>session_options</code> <code>Any</code> <code>None</code> Optional <code>ort.SessionOptions</code>."},{"location":"guides/integrations/#positional-inputs","title":"Positional inputs","text":"<p><code>predict()</code> matches positional arguments to input names in order:</p> <pre><code>results = onnx_model.predict(input_1, input_2)\n# Returns a list of output arrays\n</code></pre>"},{"location":"guides/integrations/#named-inputs","title":"Named inputs","text":"<p><code>predict_dict()</code> accepts a dictionary of named inputs and returns a dictionary of named outputs:</p> <pre><code>results = onnx_model.predict_dict({\n    \"input_ids\": input_ids_array,\n    \"attention_mask\": attention_mask_array,\n})\n# Returns {\"output_name\": array, ...}\n</code></pre>"},{"location":"guides/integrations/#introspection","title":"Introspection","text":"<pre><code>print(onnx_model.input_names)   # [\"input_ids\", \"attention_mask\"]\nprint(onnx_model.output_names)  # [\"logits\"]\n</code></pre>"},{"location":"guides/integrations/#installing-all-extras","title":"Installing all extras","text":"<p>To install all framework integrations at once:</p> <pre><code>pip install blazerpc[all]\n</code></pre>"},{"location":"guides/middleware/","title":"Middleware","text":"<p>BlazeRPC provides a middleware system built on grpclib's event hooks. Middleware runs outside the handler function and observes request/response events without modifying the request data itself.</p>"},{"location":"guides/middleware/#built-in-middleware","title":"Built-in middleware","text":"<p>BlazeRPC ships with three middleware classes:</p>"},{"location":"guides/middleware/#loggingmiddleware","title":"LoggingMiddleware","text":"<p>Logs every RPC call with the method name, peer address, and response status.</p> <pre><code>from blazerpc.server.middleware import LoggingMiddleware\n\nmiddleware = LoggingMiddleware()\nmiddleware.attach(grpclib_server)\n</code></pre> <p>You can pass a custom <code>logging.Logger</code> instance:</p> <pre><code>import logging\n\nlogger = logging.getLogger(\"my_app.rpc\")\nmiddleware = LoggingMiddleware(logger=logger)\n</code></pre>"},{"location":"guides/middleware/#metricsmiddleware","title":"MetricsMiddleware","text":"<p>Exports Prometheus metrics for every RPC call:</p> Metric Type Labels Description <code>blazerpc_requests_total</code> Counter <code>method</code>, <code>status</code> Total number of requests. <code>blazerpc_request_duration_seconds</code> Histogram <code>method</code> Request duration. <pre><code>from blazerpc.server.middleware import MetricsMiddleware\n\nmiddleware = MetricsMiddleware()\nmiddleware.attach(grpclib_server)\n</code></pre> <p>The metrics are registered as module-level Prometheus objects and can be scraped by any Prometheus-compatible collector.</p>"},{"location":"guides/middleware/#exceptionmiddleware","title":"ExceptionMiddleware","text":"<p>A base class for custom exception-to-gRPC-status mapping. The default implementation is a no-op -- subclass it to add your own mapping logic.</p>"},{"location":"guides/middleware/#writing-custom-middleware","title":"Writing custom middleware","text":"<p>Subclass <code>Middleware</code> and implement <code>on_request()</code> and <code>on_response()</code>:</p> <pre><code>from grpclib.const import Status\nfrom grpclib.events import RecvRequest, SendTrailingMetadata\nfrom grpclib.exceptions import GRPCError\n\nfrom blazerpc.server.middleware import Middleware\n\nclass AuthMiddleware(Middleware):\n    \"\"\"Reject requests without a valid authorization token.\"\"\"\n\n    async def on_request(self, event: RecvRequest) -&gt; None:\n        metadata = dict(event.metadata)\n        token = metadata.get(\"authorization\")\n        if not token or not self._validate_token(token):\n            raise GRPCError(\n                Status.UNAUTHENTICATED,\n                \"Missing or invalid authorization token\",\n            )\n\n    async def on_response(self, event: SendTrailingMetadata) -&gt; None:\n        pass  # No response-side logic needed.\n\n    def _validate_token(self, token: str) -&gt; bool:\n        return token == \"my-secret-token\"\n</code></pre> <p>Attach it to the server:</p> <pre><code>auth = AuthMiddleware()\nauth.attach(grpclib_server)\n</code></pre>"},{"location":"guides/middleware/#how-it-works","title":"How it works","text":"<p>Middleware hooks into grpclib's event system using the <code>listen()</code> function. When you call <code>middleware.attach(server)</code>, two event listeners are registered:</p> <ul> <li><code>RecvRequest</code> -- fires when a request is received, before the handler runs.</li> <li><code>SendTrailingMetadata</code> -- fires when the response is about to be sent, after the handler completes.</li> </ul> <p>This design means middleware cannot modify the request payload, but it can:</p> <ul> <li>Inspect request metadata (headers, peer address, method name).</li> <li>Reject requests by raising <code>GRPCError</code>.</li> <li>Record timing and status information for observability.</li> <li>Add trailing metadata to responses.</li> </ul>"},{"location":"guides/middleware/#attaching-multiple-middleware","title":"Attaching multiple middleware","text":"<p>Middleware is applied in the order you attach it. Each middleware's <code>on_request()</code> runs before the handler, and each <code>on_response()</code> runs after:</p> <pre><code>LoggingMiddleware().attach(server)\nMetricsMiddleware().attach(server)\nAuthMiddleware().attach(server)\n</code></pre> <p>In this example, logging fires first, then metrics, then auth. If auth rejects the request, the handler never runs, but the logging and metrics middleware still see the response event with the <code>UNAUTHENTICATED</code> status.</p>"},{"location":"guides/streaming/","title":"Streaming","text":"<p>Server-side streaming lets you send responses incrementally over an open gRPC stream. This is essential for workloads where the client should see partial results as they are produced -- for example, an LLM generating tokens one at a time.</p>"},{"location":"guides/streaming/#basic-usage","title":"Basic usage","text":"<p>Set <code>streaming=True</code> on the model decorator and write an async generator function:</p> <pre><code>import asyncio\nfrom blazerpc import BlazeApp\n\napp = BlazeApp()\n\n@app.model(\"generate\", streaming=True)\nasync def generate_tokens(prompt: str) -&gt; str:\n    tokens = run_my_llm(prompt)\n    for token in tokens:\n        await asyncio.sleep(0)  # yield control to the event loop\n        yield token\n</code></pre> <p>Each <code>yield</code> sends one message to the client over the open gRPC stream. The client receives tokens as they are produced, without waiting for the full response.</p>"},{"location":"guides/streaming/#how-it-works","title":"How it works","text":"<p>When BlazeRPC sees <code>streaming=True</code> on a model, it:</p> <ol> <li>Generates a <code>returns (stream Response)</code> RPC in the <code>.proto</code> file instead of a plain <code>returns (Response)</code>.</li> <li>Creates a streaming handler that iterates over the generator and calls <code>stream.send_message()</code> for each yielded value.</li> </ol> <p>Synchronous generators also work, but async generators are preferred because they do not block the event loop between yields.</p>"},{"location":"guides/streaming/#generated-proto","title":"Generated proto","text":"<p>A streaming model produces a proto definition like this:</p> <pre><code>service InferenceService {\n  rpc PredictGenerate(GenerateRequest) returns (stream GenerateResponse);\n}\n</code></pre> <p>Clients consume the stream by reading messages in a loop until the server closes the stream.</p>"},{"location":"guides/streaming/#client-disconnection","title":"Client disconnection","text":"<p>If the client disconnects mid-stream, grpclib raises <code>asyncio.CancelledError</code> inside the generator. BlazeRPC catches and re-raises this so the generator is properly cleaned up. If you need to run cleanup logic on disconnection, use a <code>try/finally</code> block:</p> <pre><code>@app.model(\"generate\", streaming=True)\nasync def generate_tokens(prompt: str) -&gt; str:\n    try:\n        async for token in model.generate(prompt):\n            yield token\n    finally:\n        # cleanup resources if needed\n        pass\n</code></pre>"},{"location":"guides/streaming/#when-to-use-streaming","title":"When to use streaming","text":"<p>Streaming is the right choice when:</p> <ul> <li>Response latency matters more than throughput. The client sees the first token in milliseconds instead of waiting for the full response.</li> <li>Responses are unbounded or very large. Streaming avoids buffering the entire response in memory.</li> <li>The client drives cancellation. A user can stop generation early without wasting server compute.</li> </ul> <p>For workloads where the full response is small and available immediately, a standard unary RPC is simpler and has less overhead.</p> <p>Note</p> <p>Streaming models are not compatible with adaptive batching. When <code>streaming=True</code>, requests are processed individually even if batching is enabled on the app.</p>"}]}